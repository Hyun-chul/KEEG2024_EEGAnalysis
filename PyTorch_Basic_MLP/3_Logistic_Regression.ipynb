{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Logistic_Regression.ipynb","private_outputs":true,"provenance":[{"file_id":"1_1GmzsykMbgm2yKwMY_hm1LJ0BwOWNs3","timestamp":1652429474710}],"collapsed_sections":["VvSHrdUfofvN"],"authorship_tag":"ABX9TyPxtM2obTpVBG2h5vAIyAJ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V_ithZb_hHye"},"source":["참고문헌: PyTorch로 시작하는 딥러닝 입문 (WikiDocs), 파이썬 딥러닝 파이토치 (이경택,방성수, 안상준 지음), 정보문화사"]},{"cell_type":"markdown","metadata":{"id":"q4ZoDdp9cTAb"},"source":["# 로지스틱 회귀(Logistic Regression)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NDuO_73nikiv"},"source":["일상 속 풀고자하는 많은 문제 중에서는 두 개의 선택지 중에서 정답을 고르는 문제가 많습니다. 예를 들어 시험을 봤는데 이 시험 점수가 합격인지 불합격인지가 궁금할 수도 있고, 어떤 메일을 받았을 때 이게 정상 메일인지 스팸 메일인지를 분류하는 문제도 그렇습니다. 이렇게 둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)라고 합니다. 그리고 이진 분류를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)가 있습니다.\n","\n","\n","> 로지스틱 회귀는 알고리즘의 이름은 회귀이지만 실제로는 분류(Classification) 작업에 사용할 수 있습니다.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QCRlVPr7jtHX"},"source":["# 1. 이진 분류(Binary Classification)\n","\n","학생들이 시험 성적에 따라서 합격, 불합격이 기재된 데이터가 있다고 가정해봅시다. 시험 성적이 라면, 합불 결과는 입니다. 이 시험의 커트라인은 공개되지 않았는데 이 데이터로부터 특정 점수를 얻었을 때의 합격, 불합격 여부를 판정하는 모델을 만들고자 합시다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SY3o2vODjziH"},"source":["\n","score()\t |  result()\n","--- | ---\n","45| 불합격\n","50| 불합격\n","55| 불합격\n","60| 합격\n","65| 합격\n","70| 합격\n","\n","위의 데이터에서 합격을 1, 불합격을 0이라고 하였을 때 그래프를 그려보면 아래와 같습니다."]},{"cell_type":"markdown","metadata":{"id":"4WYMdB6gmd_J"},"source":["![Image](https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"NcVIBJTTnJuH"},"source":["이러한 점들을 표현하는 그래프는 알파벳의 $S$자 형태로 표현됩니다. 이러한 $x$ 와 $y$ 의 관계를 표현하기 위해서는 $Wx + b$ 와 같은 직선 함수가 아니라 $S$자 형태로 표현할 수 있는 함수가 필요합니다. 이런 문제에 직선을 사용할 경우 분류 작업이 잘 동작하지 않습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XOILrCJUnTm2"},"source":["그래서 이번 로지스틱 회귀의 가설은 선형 회귀 때의 $H(x) = Wx + b$ 가 아니라, 위와 같이 $S$자 모양의 그래프를 만들 수 있는 어떤 특정 함수 $f$ 를 추가적으로 사용하여 $H(x) = f(Wx + b)$ 의 가설을 사용할 겁니다. 그리고 위와 같이 $S$자 모양의 그래프를 그릴 수 있는 어떤 함수 $f$가 이미 널리 알려져있습니다. 바로 시그모이드 함수입니다."]},{"cell_type":"markdown","metadata":{"id":"SpjCrIQ5n0rO"},"source":["# 2. 시그모이드 함수(Sigmoid function)\n","\n","위와 같이 S자 형태로 그래프를 그려주는 시그모이드 함수의 방정식은 아래와 같습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x_JNGiYbiy6v"},"source":["$H(x) = sigmoid(Wx + b) = \\frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)$"]},{"cell_type":"markdown","metadata":{"id":"cssPZ_75n8nj"},"source":["선형 회귀에서는 최적의 $W$ 와 $b$ 를 찾는 것이 목표였습니다. 여기서도 마찬가지입니다. 선형 회귀에서는 $W$ 가 직선의 기울기, $b$ 가 $y$절편을 의미했습니다. 그렇다면 여기에서는 $W$ 와 $b$ 가 함수의 그래프에 어떤 영향을 주는지 직접 그래프를 그려서 알아보겠습니다.\n","\n","> 파이썬에서는 그래프를 그릴 수 있는 도구로서 Matplotlib을 사용할 수 있습니다.\n"]},{"cell_type":"code","metadata":{"id":"5-eClI1DjQuc"},"source":["%matplotlib inline\n","import numpy as np # 넘파이 사용\n","import matplotlib.pyplot as plt # 맷플롯립사용\n","\n","def sigmoid(x): # 시그모이드 함수 정의\n","    return 1/(1+np.exp(-x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eK_FtGpuoVQ_"},"source":["x = np.arange(-5.0, 5.0, 0.1)\n","y = sigmoid(x)\n","\n","plt.plot(x, y, 'g')\n","plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n","plt.title('Sigmoid Function')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBvVOT4QoXvz"},"source":["위의 그래프를 통해시그모이드 함수는 출력값을 0과 1사이의 값으로 조정하여 반환함을 알 수 있습니다. $x$ 가 0일 때 0.5의 값을 가집니다. $x$ 가 매우 커지면 1에 수렴합니다. 반면, $x$ 가 매우 작아지면 0에 수렴합니다.\n"]},{"cell_type":"markdown","metadata":{"id":"VvSHrdUfofvN"},"source":["# 시그모이드 함수를 이용한 분류\n","\n","시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴합니다. **시그모이드 함수의 출력값은 0과 1 사이의 값을 가지는데**이 특성을 이용하여 분류 작업에 사용할 수 있습니다. 예를 들어 임계값을 0.5라고 정해보겠습니다. 출력값이 0.5 이상이면 1(True), 0.5이하면 0(False)으로 판단하도록 할 수 있습니다. 이를 확률이라고 생각하면 해당 레이블에 속할 확률이 50%가 넘으면 해당 레이블로 판단하고, 해당 레이블에 속할 확률이 50%보다 낮으면 아니라고 판단하는 것으로 볼 수 있습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YNkM0vpPouI8"},"source":["# 3. 비용 함수(Cost function)\n"]},{"cell_type":"markdown","metadata":{"id":"_kZM971joy8-"},"source":["이제 로지스틱 회귀의 가설이 $H(x) = sigmoid(Wx + b)$인 것은 알았습니다. 최적의 $W$ 와 $b$ 를 찾을 수 있는 비용 함수(cost function)를 정의해야 합니다. 그런데 혹시 앞서 선형 회귀에서 배운 비용 함수인 평균 제곱 오차(Mean Square Error, MSE)를 로지스틱 회귀의 비용 함수로 그냥 사용하면 안 될까요?\n","\n","다음은 선형 회귀에서 사용했던 평균 제곱 오차의 수식입니다."]},{"cell_type":"markdown","metadata":{"id":"8XTHNdxGo7Ub"},"source":["$cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$\n"]},{"cell_type":"markdown","metadata":{"id":"X5wmzToIpEWk"},"source":["위의 비용 함수 수식에서 가설은 이제 $H(x) = Wx + b$가 아니라 $H(x) = sigmoid(Wx + b)$입니다. 그리고 이 비용 함수를 미분하면 선형 회귀때와 달리 다음의 그림과 유사한 심한 비볼록(non-convex) 형태의 그래프가 나옵니다"]},{"cell_type":"markdown","metadata":{"id":"EqE0wTG3pPQC"},"source":["![Image](https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"u-Vz3m6bpYrm"},"source":["위와 같은 그래프에 경사 하강법을 사용할 경우의 문제점은 경사 하강법이 오차가 최소값이 되는 구간에 도착했다고 판단한 그 구간이 실제 오차가 완전히 최소값이 되는 구간이 아닐 수 있다는 점입니다. 사람이 등산 후에 산을 내려올 때도, 가파른 경사를 내려오다가 넓은 평지가 나오면 순간적으로 다 내려왔다고 착각할 수 있습니다. 하지만 실제로는 그곳이 다 내려온 것이 아니라 잠깐 평지가 나왔을 뿐이라면 길을 더 찾아서 더 내려가야 할 겁니다. 모델도 마찬가지로 실제 오차가 최소가 되는 구간을 찾을 수 있도록 도와주어야 합니다. 만약, 실제 최소가 되는 구간을 잘못 판단하면 최적의 가중치 W 가 아닌 다른 값을 택해 모델의 성능이 더 오르지 않습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pjbHouRqpbhB"},"source":["이를 전체 함수에 걸쳐 최소값인 글로벌 미니멈(Global Minimum)이 아닌 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum)에 도달했다고 합니다. 이는 cost가 최소가 되는 가중치 W 를 찾는다는 비용 함수의 목적에 맞지 않습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"okcJ4N0Cpelg"},"source":["시그모이드 함수의 특징은 함수의 출력값이 0과 1사이의 값이라는 점입니다. 즉, 실제값이 1일 때 예측값이 0에 가까워지면 오차가 커져야 하며, 실제값이 0일 때, 예측값이 1에 가까워지면 오차가 커져야 합니다. 그리고 이를 충족하는 함수가 바로 로그 함수입니다. 다음은 y = 0.5 에 대칭하는 두 개의 로그 함수 그래프입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lLvWbN6OpjcC"},"source":["![Image](https://wikidocs.net/images/page/57805/%EA%B7%B8%EB%9E%98%ED%94%84.PNG)\n"]},{"cell_type":"markdown","metadata":{"id":"uQ3xTRZ4pMeE"},"source":["실제값이 1일 때의 그래프를 주황색 선으로 표현하였으며, 실제값이 0일 때의 그래프를 초록색 선으로 표현하였습니다. 실제값이 1이라고 해봅시다. 이 경우, 예측값인 $H(x)$의 값이 1이면 오차가 0이므로 당연히 cost는 0이 됩니다. 반면, $H(x)$ 가 0으로 수렴하면 cost는 무한대로 발산합니다. 실제값이 0인 경우는 그 반대로 이해하면 됩니다. 이 두 개의 로그 함수를 식으로 표현하면 다음과 같습니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y1v1-mmIpu-5"},"source":["$ \\text{if } y=1 → \\text{cost}\\left( H(x), y \\right) = -\\log(H(x)) $\n","\n","$ \\text{if } y=0 → \\text{cost}\\left( H(x), y \\right) = -\\log(1-H(x)) $"]},{"cell_type":"markdown","metadata":{"id":"ArAf0JlNp3S8"},"source":["$y$ 의 실제값이 1일 때 $ -\\log(H(x)) $ 그래프를 사용하고$y$의 실제값이 0일 때 $ -\\log(1 - H(x)) $ 그래프를 사용해야 합니다. 이는 다음과 같이 하나의 식으로 통합할 수 있습니다"]},{"cell_type":"markdown","metadata":{"id":"-syCd3BOqHdS"},"source":["$ \\text{cost}\\left( H(x), y \\right) = -[ylogH(x) + (1-y)log(1-H(x))] $"]},{"cell_type":"markdown","metadata":{"id":"gUr-EyxiqMLX"},"source":["왜 위 식이 두 개의 식을 통합한 식이라고 볼 수 있을까요? 실제값 가 1이라고하면 덧셈 기호를 기준으로 우측의 항이 없어집니다. 반대로 실제값 $y$ 가 0이라고 하면 덧셈 기호를 기준으로 좌측의 항이 없어집니다. 선형 회귀에서는 모든 오차의 평균을 구해 평균 제곱 오차를 사용했었습니다. 마찬가지로 여기에서도 모든 오차의 평균을 구합니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QwcZ6v2yqQdZ"},"source":["$ cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))] $"]},{"cell_type":"markdown","metadata":{"id":"IjqTQhNXqTdD"},"source":["정리하면, 위 비용 함수는 실제값 $y$와 예측값 $H(x)$ 의 차이가 커지면 cost가 커지고, 실제값 $y$ 와 예측값 $H(x)$의 차이가 작아지면 cost는 작아집니다. 이제 위 비용 함수에 대해서 경사 하강법을 수행하면서 최적의 가중치 $W$를 찾아갑니다."]},{"cell_type":"markdown","metadata":{"id":"fjjwrevDqdzZ"},"source":["$ W := W - α\\frac{∂}{∂W}cost(W) $"]},{"cell_type":"markdown","metadata":{"id":"lTO_A_GaqtXa"},"source":["# 4. 파이토치로 로지스틱 회귀 구현하기\n"]},{"cell_type":"markdown","metadata":{"id":"Hd-4dG4lqxjR"},"source":["이제 파이토치로 로지스틱 회귀 중에서도 다수의 $x$로 부터 $y$를 예측하는 다중 로지스틱 회귀를 구현해봅시다.\n","\n","우선 필요한 도구들을 임포트합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"kTnPLplNq4Lj"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TfvrnVMYq8yX"},"source":["x_train과 y_train을 텐서로 선언합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"yX-caoPqq6-Z"},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJTpMJ0Gq-Oy"},"source":["앞서 훈련 데이터를 행렬로 선언하고, 행렬 연산으로 가설을 세우는 방법을 배웠습니다.\n","여기서도 마찬가지로 행렬 연산을 사용하여 가설식을 세울겁니다. x_train과 y_train의 크기를 확인해봅시다.\n","\n"]},{"cell_type":"code","metadata":{"id":"dZKEkeG6rA9l"},"source":["print(x_train.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HlZh0C1IrEiO"},"source":["현재 x_train은 6 × 2의 크기(shape)를 가지는 행렬이며, y_train은 6 × 1의 크기를 $X$가지는 벡터입니다. x_train을 라고 하고, 이와 곱해지는 가중치 벡터를 $W$라고 하였을 때, $XW$가 성립되기 위해서는 $W$ 벡터의 크기는 2 × 1이어야 합니다. 이제 W와 b를 선언합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"-mV_EysBrPFh"},"source":["W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n","b = torch.zeros(1, requires_grad=True)\\"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10K3JBFHrSQP"},"source":["이제 가설식을 세워보겠습니다. 파이토치에서는 $e^x$ 를 구현하기 위해서 torch.exp(x)를 사용합니다.\n","이에 따라 행렬 연산을 사용한 가설식은 다음과 같습니다."]},{"cell_type":"code","metadata":{"id":"nXCB9S3prRYW"},"source":["hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMuO56o_rY9o"},"source":["앞서 W와 b는 torch.zeros를 통해 전부 0으로 초기화 된 상태입니다. 이 상태에서 예측값을 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"kI2FsICirYhd"},"source":["print(hypothesis) # 예측값인 H(x) 출력"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kObO72nBrcz1"},"source":["실제값 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.\n","\n","사실 가설식을 좀 더 간단하게도 구현할 수 있습니다. 이미 파이토치에서는 시그모이드 함수를 이미 구현하여 제공하고 있기 때문입니다. 다음은 torch.sigmoid를 사용하여 좀 더 간단히 구현한 가설식입니다.\n"]},{"cell_type":"code","metadata":{"id":"Qxls7Zv-rfNX"},"source":["hypothesis = torch.sigmoid(x_train.matmul(W) + b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qiV4Oevrge7"},"source":["앞서 구현한 식과 본질적으로 동일한 식입니다. 마찬가지로 W와 b가 0으로 초기화 된 상태에서 예측값을 출력해봅시다."]},{"cell_type":"code","metadata":{"id":"NU_uoz_HrlWI"},"source":["print(hypothesis)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8WBXArYrk2L"},"source":["앞선 결과와 동일하게 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.\n","\n","이제 아래의 비용 함수값. 즉, 현재 예측값과 실제값 사이의 cost를 구해보겠습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"d3u0p5fOrqcK"},"source":["$ cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))] $"]},{"cell_type":"markdown","metadata":{"id":"7_yuh1iAruhc"},"source":["우선, 현재 예측값과 실제값을 출력해보겠습니다."]},{"cell_type":"code","metadata":{"id":"64mV3zWyrwup"},"source":["print(hypothesis)\n","print(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aO28OUz9ryjf"},"source":["현재 총 6개의 원소가 존재하지만 하나의 샘플. 즉, 하나의 원소에 대해서만 오차를 구하는 식을 작성해보겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"biC6sZcfr0Zx"},"source":["-(y_train[0] * torch.log(hypothesis[0]) + \n","  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYAaHg7Jr2Xb"},"source":["이제 모든 원소에 대해서 오차를 구해보겠습니다."]},{"cell_type":"code","metadata":{"id":"OaD4RMUOr34I"},"source":["losses = -(y_train * torch.log(hypothesis) + \n","           (1 - y_train) * torch.log(1 - hypothesis))\n","print(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgOZo7tVr5DC"},"source":["그리고 이 전체 오차에 대한 평균을 구합니다."]},{"cell_type":"code","metadata":{"id":"821YEHT7r6Th"},"source":["cost = losses.mean()\n","print(cost)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8jfz7QYgr8ku"},"source":["결과적으로 얻은 cost는 0.6931입니다.\n","\n","지금까지 비용 함수의 값을 직접 구현하였는데, 사실 파이토치에서는 로지스틱 회귀의 비용 함수를 이미 구현해서 제공하고 있습니다.\n","사용 방법은 torch.nn.functional as F와 같이 임포트 한 후에 F.binary_cross_entropy(예측값, 실제값)과 같이 사용하면 됩니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"A3dB9bnrr-XC"},"source":["F.binary_cross_entropy(hypothesis, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHDbyEgMsAj9"},"source":["동일하게 cost가 0.6931이 출력되는 것을 볼 수 있습니다. 모델의 훈련 과정까지 추가한 전체 코드는 아래와 같습니다."]},{"cell_type":"code","metadata":{"id":"K6DIXWuisFgZ"},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTBswsHBsG4L"},"source":["# 모델 초기화\n","W = torch.zeros((2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산\n","    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","    cost = -(y_train * torch.log(hypothesis) + \n","             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad() #\n","    cost.backward() #\n","    optimizer.step() # \n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-wSHdAosIJe"},"source":["학습이 끝났습니다. 이제 훈련했던 훈련 데이터를 그대로 입력으로 사용했을 때, 제대로 예측하는지 확인해보겠습니다.\n","현재 W와 b는 훈련 후의 값을 가지고 있습니다. 현재 W와 b를 가지고 예측값을 출력해보겠습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"3elp_KA6sJNI"},"source":["hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","print(hypothesis)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qj_sUPzMsKwu"},"source":["현재 위 값들은 0과 1 사이의 값을 가지고 있습니다. 이제 0.5를 넘으면 True, 넘지 않으면 False로 값을 정하여 출력해보겠습니다."]},{"cell_type":"code","metadata":{"id":"laGwfZ9qsMLe"},"source":["prediction = hypothesis >= torch.FloatTensor([0.5])\n","print(prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xppztRPDsNRw"},"source":["실제값은 [[0], [0], [0], [1], [1], [1]]이므로, 이는 결과적으로 False, False, False, True, True, True와 동일합니다. 즉, 기존의 실제값과 동일하게 예측한 것을 볼 수 있습니다. 훈련이 된 후의 W와 b의 값을 출력해보겠습니다."]},{"cell_type":"code","metadata":{"id":"dGN7QZu3sOuh"},"source":["print(W)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwTVybLntH8i"},"source":["# 파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기"]},{"cell_type":"code","metadata":{"id":"1vPmQ3aUtKYl"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)\n","\n","x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPigl4OItUfr"},"source":["nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 합니다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 좋습니다. 조금 쉽게 말해서 nn.Sequential()은 $Wx+b$와 같은 수식과 시그모이드 함수 등과 같은 여러 함수들을 연결해주는 역할을 합니다. 이를 이용해서 로지스틱 회귀를 구현해봅시다.\n"]},{"cell_type":"code","metadata":{"id":"NzQi8_urtQ91"},"source":["model = nn.Sequential(\n","   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n","   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f13QrWq5taaT"},"source":["현재 W와 b는 랜덤 초기화가 된 상태입니다. 훈련 데이터를 넣어 예측값을 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"KhHj2jyLtbol"},"source":["model(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4F9IfsqtdBe"},"source":["6 × 1 크기의 예측값 텐서가 출력됩니다. 그러나 현재 W와 b는 임의의 값을 가지므로 현재의 예측은 의미가 없습니다. 이제 경사 하강법을 사용하여 훈련해보겠습니다. 총 100번의 에포크를 수행합니다."]},{"cell_type":"code","metadata":{"id":"6xkaxjljtfK1"},"source":["# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = model(x_train)\n","\n","    # cost 계산\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 20번마다 로그 출력\n","    if epoch % 10 == 0:\n","        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n","        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n","        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n","        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n","            epoch, nb_epochs, cost.item(), accuracy * 100,\n","        ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bx9EpcQythhT"},"source":["중간부터 정확도는 100%가 나오기 시작합니다. 기존의 훈련 데이터를 입력하여 예측값을 확인해보겠습니다."]},{"cell_type":"code","metadata":{"id":"Q5oUgStPtjC7"},"source":["model(x_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHF_YyFqtkOk"},"source":["0.5를 넘으면 True, 그보다 낮으면 False로 간주합니다. 실제값은 [[0], [0], [0], [1], [1], [1]]입니다. 이는 False, False, False, True, True, True에 해당되므로 전부 실제값과 일치하도록 예측한 것을 확인할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"xQX-5L-itlsI"},"source":["print(list(model.parameters()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZ0a9sVLTPfc"},"source":["## 클래스로 파이토치 모델 구현하기: Logistic regression\n"]},{"cell_type":"markdown","metadata":{"id":"vpYYIDxkTSV4"},"source":["# 1. 모델을 클래스로 구현하기\n","\n","앞서 로지스틱 회귀 모델은 다음과 같이 구현했었습니다."]},{"cell_type":"markdown","metadata":{"id":"KsbNiA9RTXZ0"},"source":["```\n","model = nn.Sequential(\n","   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n","   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",")\n","```"]},{"cell_type":"markdown","metadata":{"id":"eab_RFJ_TcgR"},"source":["이를 클래스로 구현하면 다음과 같습니다."]},{"cell_type":"markdown","metadata":{"id":"kRR8TD3wTe1g"},"source":["```\n","class BinaryClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(2, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.sigmoid(self.linear(x)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"enLZ1AZkTg3p"},"source":["위와 같은 클래스를 사용한 모델 구현 형식은 대부분의 파이토치 구현체에서 사용하고 있는 방식으로 반드시 숙지할 필요가 있습니다.\n","\n","클래스(class) 형태의 모델은 nn.Module 을 상속받습니다. 그리고 __init__()에서 모델의 구조와 동적을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다. foward() 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수입니다. 이 forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. 예를 들어 model이란 이름의 객체를 생성 후, model(입력 데이터)와 같은 형식으로 객체를 호출하면 자동으로 forward 연산이 수행됩니다.\n"]},{"cell_type":"markdown","metadata":{"id":"CTenF5HJTm25"},"source":[" > $H(x)$식에 입력 $x$로부터 예측된 $y$를 얻는 것을 forward 연산이라고 합니다."]},{"cell_type":"markdown","metadata":{"id":"Y_BH90HfTpZ-"},"source":["# 2. 로지스틱 회귀 클래스로 구현하기\n","이제 모델을 클래스로 구현한 코드를 보겠습니다. 달라진 점은 모델을 클래스로 구현했다는 점 뿐입니다. 다른 코드는 전부 동일합니다."]},{"cell_type":"code","metadata":{"id":"8ZZy_ZHxTrLt"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8S9wdkjTszq"},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TbpD0xETuMF"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(2, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return self.sigmoid(self.linear(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AO5aeXqmNOSH"},"source":["model = BinaryClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4sG9sWYTvTd"},"source":["# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = model(x_train)\n","\n","    # cost 계산\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 20번마다 로그 출력\n","    if epoch % 10 == 0:\n","        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n","        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n","        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n","        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n","            epoch, nb_epochs, cost.item(), accuracy * 100,\n","        ))"],"execution_count":null,"outputs":[]}]}