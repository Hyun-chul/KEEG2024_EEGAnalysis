{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_Linear_Regression.ipynb","private_outputs":true,"provenance":[{"file_id":"1HKCG6CPRRfLl2aOZWB1LISYlcyszbHA9","timestamp":1652408645124}],"collapsed_sections":[],"authorship_tag":"ABX9TyPlslkSsBl3ZZ9RMEh6sFHp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E8kf9_hgWnrV"},"source":["참고문헌: PyTorch로 시작하는 딥 러닝 입문 (wikidocs), 파이썬 딥러닝 파이토치 (이경택,방성수, 안상준 지음), 정보문화사\n"]},{"cell_type":"markdown","metadata":{"id":"L_WYF9zZJD4V"},"source":["# 선형 회귀(Linear Regression)\n","---\n","\n","이번 챕터에서는 선형 회귀 이론에 대해서 이해하고, 파이토치(PyTorch)를 이용하여 선형 회귀 모델을 만들어보겠습니다.\n","\n","<img src = \"https://raw.githubusercontent.com/Hyun-chul/KAIA2022/main/Regression.png\">\n","\n","*  데이터에 대한 이해(Data Definition)\n","*  학습할 데이터에 대해서 알아봅니다.\n","\n","\n","*  가설(Hypothesis) 수립\n","*  가설을 수립하는 방법에 대해서 알아봅니다.\n","\n","\n","*  손실 계산하기(Compute loss)\n","*  학습 데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때 손실(loss)를 이용합니다.\n","\n","\n","*  경사 하강법(Gradient Descent)\n","*  학습을 위한 핵심 알고리즘인 경사 하강법(Gradient Descent)에 대해서 이해합니다.\n"]},{"cell_type":"markdown","metadata":{"id":"71cWFoR-JzYW"},"source":["- 데이터에 대한 이해(Data Definition)\n","\n","    이번 챕터에서 선형 회귀를 위해 사용할 예제는 공부한 시간과 점수에 대한 상관관계입니다.\n","\n","-  훈련 데이터셋과 테스트 데이터셋\n","\n","    어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부했더니 6점을 맞았습니다. 그렇다면, 내가 4시간을 공부한다면 몇 점을 맞을 수 있을까요?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"je9xm3-zNs4D"},"source":["1. 기본 셋팅"]},{"cell_type":"code","metadata":{"id":"6G5X7rQxNglo"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxBWeVOZNkFw"},"source":["# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tuub9FlANmkp"},"source":["실습을 위한 기본적인 셋팅이 끝났습니다. 이제 훈련 데이터인 x_train과 y_train을 선언합니다."]},{"cell_type":"markdown","metadata":{"id":"5kMOM0QsNzGa"},"source":["2. 변수 선언\n"]},{"cell_type":"code","metadata":{"id":"PpxVUaM1N7TH"},"source":["x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sB4QV_KcOAI3"},"source":["x_train과 x_train의 크기(shape)를 출력해보겠습니다."]},{"cell_type":"code","metadata":{"id":"dQLndQVHOD_r"},"source":["print(x_train)\n","print(x_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10hkOGupOMe9"},"source":["x_train의 값이 출력되고, x_train의 크기가 (3 × 1)임을 알 수 있습니다. \n","\n","y_train과 y_train의 크기(shape)를 출력해보겠습니다."]},{"cell_type":"code","metadata":{"id":"GgM-Z-1EOQQq"},"source":["print(y_train)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CThpT-sgOTxb"},"source":["y_train의 값이 출력되고, y_train의 크기가 (3 × 1)임을 알 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"oCA0iUTDOV8N"},"source":["3. 가중치와 편향의 초기화\n","\n","선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.\n","\n","그리고 가장 잘 맞는 직선을 정의하는 것은 바로 *W* 와 *b* 입니다. \n","\n","선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 *W* 와 *b* 값을 찾는 것입니다.\n","\n","우선 가중치 *W* 를 0으로 초기화하고, 이 값을 출력해보겠습니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"ppBEGsKDOtCw"},"source":["# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n","W = torch.zeros(1, requires_grad=True) \n","# 가중치 W를 출력\n","print(W) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcjmrkNjOw5i"},"source":["가중치 *W* 가 0으로 초기화되어있으므로 0이 출력된 것을 확인할 수 있습니다. 위에서 requires_grad=True가 인자로 주어진 것을 확인할 수 있습니다. 이는 이 변수는 학습을 통해 계속 값이 변경되는 변수임을 의미합니다."]},{"cell_type":"markdown","metadata":{"id":"gNAoQ48NOzpH"},"source":["마찬가지로 편향 *b*도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시합니다."]},{"cell_type":"code","metadata":{"id":"iPYPu5lBO9Av"},"source":["b = torch.zeros(1, requires_grad=True)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDiz6M6iPAn5"},"source":["현재 가중치 $W$와 $b$ 둘 다 0이므로 현 직선의 방정식은 다음과 같습니다.\n","\n","$y = Wx + b$\n","\n","지금 상태에선 $x$에 어떤 값이 들어가도 가설은 0을 예측하게 됩니다. 즉, 아직 적절한 $W$와 $b$의 값이 아닙니다."]},{"cell_type":"markdown","metadata":{"id":"yEiI34ydPMkC"},"source":["4. 가설 세우기\n"]},{"cell_type":"markdown","metadata":{"id":"LMlmfmJnPPNf"},"source":["파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언합니다.\n","\n","$H(x) = Wx + b$"]},{"cell_type":"code","metadata":{"id":"ApCDdxtRPbkK"},"source":["hypothesis = x_train * W + b\n","print(hypothesis)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRrgpSMAPcxi"},"source":["5. 비용 함수 선언하기\n"]},{"cell_type":"markdown","metadata":{"id":"j1fm-fhFPd0i"},"source":["파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언합니다.\n"]},{"cell_type":"markdown","source":["$cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$"],"metadata":{"id":"M5V-bOOBNnI9"}},{"cell_type":"code","metadata":{"id":"TFXPm0pZPoel"},"source":["# 앞서 배운 torch.mean으로 평균을 구한다.\n","cost = torch.mean((hypothesis - y_train) ** 2) \n","print(cost)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQVAxkb7Pu9b"},"source":["6. 경사 하강법 (Gradient descent) 구현하기"]},{"cell_type":"markdown","metadata":{"id":"Ok7Db7orPwcj"},"source":["이제 경사 하강법을 구현합니다. 아래의 'stochastic gradient descent(SGD)'는 경사 하강법의 일종입니다. lr은 학습률(learning rate, $\\alpha$)를 의미합니다.\n","학습 대상인 W와 b가 SGD의 입력이 됩니다."]},{"cell_type":"markdown","source":["<figure>\n","<img src = \"https://raw.githubusercontent.com/Hyun-chul/KAIA2022/main/GD.png\"/, height = 200, width = 300>\n","</figure>\n"],"metadata":{"id":"8FZWzn9pRaId"}},{"cell_type":"markdown","source":["$H(x) = Wx + b$\n"],"metadata":{"id":"smDJ_gKKay9O"}},{"cell_type":"markdown","source":["$cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$"],"metadata":{"id":"NkxPMOfdbuvg"}},{"cell_type":"markdown","source":["기울기 = $\\frac{\\partial cost(W)}{\\partial W}$\t"],"metadata":{"id":"JvTKdzk9bsFk"}},{"cell_type":"markdown","source":["$ W := W - α\\frac{\\partial cost(W)}{\\partial W} $"],"metadata":{"id":"qiCGkch2a_Zb"}},{"cell_type":"markdown","metadata":{"id":"zgqK7i49QI3n"},"source":["> optimizer = optim.SGD([W, b], lr=0.01)"]},{"cell_type":"markdown","metadata":{"id":"icliy-SuP17B"},"source":["optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화합니다. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다. \n","\n","그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됩니다. 그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트합니다.\n"]},{"cell_type":"markdown","metadata":{"id":"i4RPyJ41QQw0"},"source":["#\n","gradient를 0으로 초기화\n","\n","> optimizer.zero_grad()\n","\n","# \n","비용 함수를 미분하여 gradient 계산\n","> cost.backward() \n","\n","# \n","W와 b를 업데이트\n","\n","> optimizer.step() "]},{"cell_type":"markdown","metadata":{"id":"cYW8kfI-Qzhm"},"source":["7. 전체 코드"]},{"cell_type":"code","metadata":{"id":"fu2CWZdNQ17P"},"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JArYvPRNQ8G6"},"source":["import matplotlib.pyplot as plt\n","\n","overlapping = 0.150\n","\n","print(hypothesis)\n","# print(hypothesis.detach())\n","\n","# prect_y = hypothesis.detach() #  기존 Tensor에서 gradient 전파가 안되는 텐서 생성\n","prect_y = hypothesis #  기존 Tensor에서 gradient 전파가 안되는 텐서 생성\n","\n","line1 = plt.plot(prect_y.detach().numpy(), c='red', alpha=overlapping, lw=5)\n","line2 = plt.plot(y_train, c='green', alpha=overlapping, lw=5)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l41iSBqiT7xy"},"source":["8. 전체 코드 (실시간 결과 보기)"]},{"cell_type":"code","metadata":{"id":"ECsGXut2TR9C"},"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))\n","        prect_y = hypothesis.detach() # 기존 Tensor에서 gradient 전파가 안되는 텐서 생성\n","\n","        line1 = plt.plot(prect_y, c='red', alpha=overlapping,lw=5)\n","        line2 = plt.plot(y_train, c='green', alpha=overlapping,lw=5)\n","\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R70zs1JnUBcx"},"source":["9. 전체 코드 (에러값 저장하기)"]},{"cell_type":"code","metadata":{"id":"1iE2ZkkdT0eD"},"source":["# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.0001)\n","\n","nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n","\n","# cost 값을 저장할 수 있는 variable 선언\n","cost_vals = torch.zeros(nb_epochs+1)\n","print(cost_vals)\n","\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    cost_vals[epoch] = cost.item()\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9b1vn99U2I9"},"source":["print(cost_vals)\n","\n","line1 = plt.plot(cost_vals, c='red',lw=5)\n","plt.show()"],"execution_count":null,"outputs":[]}]}